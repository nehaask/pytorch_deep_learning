{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f479af5",
   "metadata": {},
   "source": [
    "### Workflow of a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "396435a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch and matplotlib\n",
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb0a525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20c480",
   "metadata": {},
   "source": [
    "## 1.1 DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b804a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:tensor([[0.0000],\n",
      "        [0.0200],\n",
      "        [0.0400],\n",
      "        [0.0600],\n",
      "        [0.0800],\n",
      "        [0.1000],\n",
      "        [0.1200],\n",
      "        [0.1400],\n",
      "        [0.1600],\n",
      "        [0.1800]]), y: tensor([[0.3000],\n",
      "        [0.3140],\n",
      "        [0.3280],\n",
      "        [0.3420],\n",
      "        [0.3560],\n",
      "        [0.3700],\n",
      "        [0.3840],\n",
      "        [0.3980],\n",
      "        [0.4120],\n",
      "        [0.4260]])\n"
     ]
    }
   ],
   "source": [
    "# Create some data using Linear Regression formula of y = mX + c \n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create range values\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# Create X and y (features and labels)\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will pop up\n",
    "y = weight * X + bias\n",
    "\n",
    "print(f'X:{X[:10]}, y: {y[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbdb2f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41b810",
   "metadata": {},
   "source": [
    "## 1.2 BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b041c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Pytorch Linear model \n",
    "class LinearRegressionModel_V2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # use nn.Linear() for creating the model parameters\n",
    "        # also called: linear transform, probing layer, fully connected layer, dense layer\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "#     def __init__(self):\n",
    "#         super().__init__() \n",
    "#         self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "#                                                 dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "#                                    requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "#         self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "#                                             dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "#                                 requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "#     # Forward defines the computation in the model\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "#         return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7b72d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModel_V2(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set manual seed\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModel_V2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c45240fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), # parameters of target model to optimize\n",
    "                            lr=0.001) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d3428",
   "metadata": {},
   "source": [
    "## 1.3 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MAE Train Loss: 0.5551779866218567 | MAE Test Loss: 0.5861001014709473 \n",
      "Epoch: 10 | MAE Train Loss: 0.543657124042511 | MAE Test Loss: 0.5726292729377747 \n",
      "Epoch: 20 | MAE Train Loss: 0.5321362614631653 | MAE Test Loss: 0.559158444404602 \n",
      "Epoch: 30 | MAE Train Loss: 0.5206154584884644 | MAE Test Loss: 0.545687735080719 \n",
      "Epoch: 40 | MAE Train Loss: 0.5090945959091187 | MAE Test Loss: 0.5322169065475464 \n",
      "Epoch: 50 | MAE Train Loss: 0.49757370352745056 | MAE Test Loss: 0.5187460780143738 \n",
      "Epoch: 60 | MAE Train Loss: 0.48605290055274963 | MAE Test Loss: 0.5052752494812012 \n",
      "Epoch: 70 | MAE Train Loss: 0.47453203797340393 | MAE Test Loss: 0.49180445075035095 \n",
      "Epoch: 80 | MAE Train Loss: 0.4630111753940582 | MAE Test Loss: 0.47833362221717834 \n",
      "Epoch: 90 | MAE Train Loss: 0.4514903128147125 | MAE Test Loss: 0.4648628234863281 \n",
      "Epoch: 100 | MAE Train Loss: 0.4399694800376892 | MAE Test Loss: 0.4513920247554779 \n",
      "Epoch: 110 | MAE Train Loss: 0.4284486174583435 | MAE Test Loss: 0.4379211962223053 \n",
      "Epoch: 120 | MAE Train Loss: 0.4169278144836426 | MAE Test Loss: 0.4244503974914551 \n",
      "Epoch: 130 | MAE Train Loss: 0.4054069519042969 | MAE Test Loss: 0.41097959876060486 \n",
      "Epoch: 140 | MAE Train Loss: 0.39388611912727356 | MAE Test Loss: 0.39750874042510986 \n",
      "Epoch: 150 | MAE Train Loss: 0.38236528635025024 | MAE Test Loss: 0.38403797149658203 \n",
      "Epoch: 160 | MAE Train Loss: 0.37084442377090454 | MAE Test Loss: 0.37056711316108704 \n",
      "Epoch: 170 | MAE Train Loss: 0.35932356119155884 | MAE Test Loss: 0.3570963740348816 \n",
      "Epoch: 180 | MAE Train Loss: 0.3478027284145355 | MAE Test Loss: 0.343625545501709 \n",
      "Epoch: 190 | MAE Train Loss: 0.3362818658351898 | MAE Test Loss: 0.3301547169685364 \n"
     ]
    }
   ],
   "source": [
    "# an epoch is one loop through the data... Set the number of epochs (hyperparameter because we set the value)\n",
    "epochs = 200\n",
    "\n",
    "### TRAINING \n",
    "\n",
    "# 0. Loop through the data\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # set model in training mode (this is the default state of a model)\n",
    "    # it sets all paramters that require gradient descent to require gradients\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward pass on train data using the forward() method inside \n",
    "    y_pred = model_1(X_train)\n",
    "\n",
    "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad of the optimizer - not for first time; 2nd epoch onwards - \n",
    "    # we need to reset the optimiser gradients every epoch; start fresh each forward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards - perform backpropagation on the loss wrt the parameters of the model (nn.Parameter mein wherever it is requires_grad = True)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Progress the optimizer (perform gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    ### Testing\n",
    "    # turns off the dfferent settings in the model which are not needed for evaluation/testing (batch norm layers/ dropout)\n",
    "    model_1.eval()\n",
    "\n",
    "    # turns off gradient descent\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_1(X_test)\n",
    "  \n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51c99691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.6865]])),\n",
       "             ('linear_layer.bias', tensor([0.6300]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f83189ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37101d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
