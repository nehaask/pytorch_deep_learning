{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f479af5",
   "metadata": {},
   "source": [
    "### Workflow of a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396435a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and matplotlib\n",
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device-agnostic code \n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20c480",
   "metadata": {},
   "source": [
    "## 1.1 DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data using Linear Regression formula of y = mX + c \n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create range values\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# Create X and y (features and labels)\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will pop up\n",
    "y = weight * X + bias\n",
    "\n",
    "print(f'X:{X[:10]}, y: {y[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41b810",
   "metadata": {},
   "source": [
    "## 1.2 BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Pytorch Linear model \n",
    "class LinearRegressionModel_V2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # use nn.Linear() for creating the model parameters\n",
    "        # also called: linear transform, probing layer, fully connected layer, dense layer\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "#     def __init__(self):\n",
    "#         super().__init__() \n",
    "#         self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "#                                                 dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "#                                    requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "#         self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "#                                             dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "#                                 requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "#     # Forward defines the computation in the model\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "#         return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b72d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModel_V2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45240fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), # parameters of target model to optimize\n",
    "                            lr=0.001) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d3428",
   "metadata": {},
   "source": [
    "## 1.3 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an epoch is one loop through the data... Set the number of epochs (hyperparameter because we set the value)\n",
    "epochs = 200\n",
    "\n",
    "### TRAINING \n",
    "\n",
    "# 0. Loop through the data\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # set model in training mode (this is the default state of a model)\n",
    "    # it sets all paramters that require gradient descent to require gradients\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward pass on train data using the forward() method inside \n",
    "    y_pred = model_1(X_train)\n",
    "\n",
    "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad of the optimizer - not for first time; 2nd epoch onwards - \n",
    "    # we need to reset the optimiser gradients every epoch; start fresh each forward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards - perform backpropagation on the loss wrt the parameters of the model (nn.Parameter mein wherever it is requires_grad = True)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Progress the optimizer (perform gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    ### Testing\n",
    "    # turns off the dfferent settings in the model which are not needed for evaluation/testing (batch norm layers/ dropout)\n",
    "    model_1.eval()\n",
    "\n",
    "    # turns off gradient descent\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_1(X_test)\n",
    "  \n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c99691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83189ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36317a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37101d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "torch.manual_seed(42)\n",
    "# This uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n",
    "                         out_features=6) # out_features = describes outer value \n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor \n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b93904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
